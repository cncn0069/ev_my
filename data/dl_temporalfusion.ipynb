{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e92b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import (\n",
    "    TemporalFusionTransformer,\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer,\n",
    "    MAE,\n",
    "    QuantileLoss\n",
    ")\n",
    "\n",
    "group_cols = [\"station_location\", \"evse_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7730f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('data_full.csv')\n",
    "\n",
    "num_cols = data_full.select_dtypes(include=[\"number\"]).columns\n",
    "data_full[num_cols] = data_full[num_cols].fillna(0)\n",
    "\n",
    "\n",
    "datetime_cols = [\n",
    "    \"connection_start_time_ts\", \"last_charge_end_time_ts\", \"charging_end_time_ts\",\n",
    "    \"connection_end_time_ts\", \"expected_departure_time_ts\"\n",
    "]\n",
    "\n",
    "for col in datetime_cols:\n",
    "    if col in data_full.columns and data_full[col].dtype == 'object':\n",
    "        data_full[col] = pd.to_datetime(data_full[col])\n",
    "        data_full[col] = data_full[col].astype('int64')\n",
    "# data_full['duration_per_kwh_missing'].fillna('Missing')\n",
    "# data_full['kwh_per_usage_time_missing'].fillna('Missing')\n",
    "# data_full['evse_type_y'].fillna('Missing')\n",
    "# data_full['supports_discharge_y'].fillna('Missing')\n",
    "# data_full['scheduled_charge'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c0df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 그룹별로 시간 순서대로 정수 인덱스 생성\n",
    "data_full = data_full.sort_values(group_cols + [\"store_timestamp\"]).copy()\n",
    "data_full[\"time_idx\"] = data_full.groupby(group_cols).cumcount().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ffd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 인덱스 및 학습/검증 분할 기준\n",
    "data_full[\"store_timestamp_dt\"] = pd.to_datetime(data_full[\"store_timestamp\"])\n",
    "\n",
    "# 2. max_prediction_length: 30일 * 24시간 * 2 (60일)\n",
    "max_prediction_length_hours = 30 * 24 * 2\n",
    "max_prediction_length_td = pd.Timedelta(hours=max_prediction_length_hours)\n",
    "\n",
    "# 3. 학습/검증 분할 기준 시점 계산\n",
    "training_cutoff = data_full[\"store_timestamp_dt\"].max() - max_prediction_length_td\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a8ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = group_cols + [\"station_location\", \"evse_name\", \"evse_type\", \"supports_discharge\", \"month\", \"weekday\"]\n",
    "\n",
    "# 전체 데이터를 문자열로 변환\n",
    "for col in cat_cols:\n",
    "    if col in data_full.columns:\n",
    "        data_full[col] = data_full[col].astype(str)\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "training_data = data_full[data_full[\"store_timestamp_dt\"] <= training_cutoff].copy()\n",
    "validation_data = data_full[data_full[\"store_timestamp_dt\"] > training_cutoff].copy()\n",
    "\n",
    "# 검증 데이터에서 학습 데이터에 없는 범주 제거\n",
    "for col in group_cols + [\"station_location\", \"evse_name\", \"evse_type\", \"supports_discharge\"]:\n",
    "    train_vals = set(training_data[col].unique())\n",
    "    validation_data = validation_data[validation_data[col].isin(train_vals)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7999e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터에만 있지만 학습 데이터에는 없는 station_location 범주: set()\n",
      "검증 데이터에만 있지만 학습 데이터에는 없는 evse_name 범주: set()\n",
      "검증 데이터에만 있지만 학습 데이터에는 없는 station_location 범주: set()\n",
      "검증 데이터에만 있지만 학습 데이터에는 없는 evse_name 범주: set()\n",
      "검증 데이터에만 있지만 학습 데이터에는 없는 evse_type 범주: set()\n",
      "검증 데이터에만 있지만 학습 데이터에는 없는 supports_discharge 범주: set()\n"
     ]
    }
   ],
   "source": [
    "for col in group_cols + [\"station_location\", \"evse_name\", \"evse_type\", \"supports_discharge\"]:\n",
    "    print(f\"검증 데이터에만 있지만 학습 데이터에는 없는 {col} 범주:\",\n",
    "          set(validation_data[col].unique()) - set(training_data[col].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd5a7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full[\"store_timestamp_dt\"] = pd.to_datetime(data_full[\"store_timestamp\"])\n",
    "data_full = data_full.sort_values(group_cols + [\"store_timestamp_dt\"]).copy()\n",
    "data_full[\"time_idx\"] = data_full.groupby(group_cols).cumcount().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66fc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30분 단위 타임스텝 기준\n",
    "max_encoder_length = 24 * 2  # 24시간 * 2 타임스텝/시간 = 48 타임스텝 (과거 24시간)\n",
    "min_encoder_length = max_encoder_length // 2  # 최소 12시간 (24 타임스텝)\n",
    "\n",
    "max_prediction_length = 30 * 24 * 2  # 30일 * 24시간 * 2 타임스텝 = 1440 타임스텝 (30일 예측 기간)\n",
    "min_prediction_length = 1  # 최소 1 타임스텝 예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ce678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\user\\.conda\\envs\\largeGarbage\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\user\\.conda\\envs\\largeGarbage\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 4      | train\n",
      "3  | prescalers                         | ModuleDict                      | 496    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 17.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 528    | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "36.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.7 K    Total params\n",
      "0.147     Total estimated model params size (MB)\n",
      "641       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502a223b341c42be9f325a97b4f9a3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\largeGarbage\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\user\\.conda\\envs\\largeGarbage\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98942d2eb7ff4018a323497973183c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------\n",
    "# TimeSeriesDataSet 생성\n",
    "# --------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    training_data[lambda x: x.store_timestamp_dt <= training_cutoff],\n",
    "    time_idx=\"time_idx\",                   # datetime 컬럼 대신 정수 인덱스 사용\n",
    "    target=\"requested_kwh\",\n",
    "    group_ids=group_cols,\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"station_location\", \"evse_name\", \"evse_type\", \"supports_discharge\"],\n",
    "    static_reals=[\"usage_departure_range\", \"post_charge_departure_range\", \"cluster\"] if \"usage_departure_range\" in data_full.columns else [],\n",
    "    # time_varying_known_categoricals=[\"month\", \"weekday\"],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        col for col in [\n",
    "            \"connection_start_time_ts\",\"last_charge_end_time_ts\", \"charging_end_time_ts\", \"connection_end_time_ts\", \"expected_departure_time_ts\",\n",
    "            \"expected_departure_time_missing\", \"idle_time_ts\", \"expected_usage_duration_ts\", \"expected_usage_duration_missing\",\n",
    "            \"expected_time_diff_ts\", \"expected_time_diff_missing\", \"actual_usage_duration_ts\", \"actual_charging_duration_ts\",\n",
    "            \"actual_charging_duration_missing\", \"start_delay_duration_ts\", \"start_delay_duration_missing\",\n",
    "            \"post_charge_departure_delay_ts\", \"post_charge_departure_delay_missing\",\n",
    "            \"usage_departure_time_diff_ts\", \"usage_departure_time_diff_missing\",\n",
    "            \"delivered_kwh\", \"requested_kwh\", \"kwh_request_diff\", \"kwh_per_usage_time\"\n",
    "        ] if col in data_full.columns\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(groups=group_cols, transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "\n",
    "# --------------------\n",
    "# Dataloader 설정\n",
    "# --------------------\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=4)\n",
    "\n",
    "# --------------------\n",
    "# Trainer & 모델 정의\n",
    "# --------------------\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"cuda\",\n",
    "    gradient_clip_val=0.1,\n",
    "    enable_model_summary=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# 학습\n",
    "# --------------------\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# 예측 및 평가\n",
    "# --------------------\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cuda\"))\n",
    "print(\"MAE:\", MAE()(predictions.output, predictions.y).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34.9355, 37.9210, 35.8729, 40.0564, 34.3832, 34.4398, 39.4118, 36.7339,\n",
      "         25.6542, 37.7875, 36.5588, 36.3540, 38.3027, 37.4756, 40.3747, 32.6620,\n",
      "         36.2315, 37.2804, 22.2992, 39.5477]], device='cuda:0')\n",
      "(tensor([[18.5000, 22.2100, 19.8400, 34.0100, 43.8200, 15.5000, 27.9900, 34.2600,\n",
      "         25.9500, 40.6600, 20.3700, 26.1900, 12.6100, 22.2000, 10.8500, 11.8000,\n",
      "         15.8100, 17.9500, 12.2500, 28.5100]], device='cuda:0'), None)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(predictions.output)\n",
    "print(predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599c587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "largeGarbage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
