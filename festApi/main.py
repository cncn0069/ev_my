# main.py

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import joblib
import uvicorn

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
from sklearn.preprocessing import RobustScaler  # ë˜ëŠ” StandardScaler ë“±
from pytorch_forecasting import (
    TemporalFusionTransformer,
    TimeSeriesDataSet,
    GroupNormalizer,
    MAE,
    QuantileLoss
)

# ----------------------------------------
# âœ… LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        return self.fc(h_n.squeeze(0))
class MLPModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, output_size=1, dropout=0.1):
        super(MLPModel, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, output_size)
        )

    def forward(self, x):
        return self.model(x)


# ----------------------------------------
# ğŸ“¦ ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
class EVChargeAPIInput(BaseModel):
    # ìˆ˜ì¹˜í˜•
    last_charge_end_time_ts: float
    connection_start_time_ts: float
    charging_start_time_ts: float
    charging_end_time_ts: float
    connection_end_time_ts: float
    expected_departure_time_ts: float
    idle_time_ts: float
    expected_usage_duration_ts: float
    expected_time_diff_ts: float
    actual_usage_duration_ts: float
    actual_charging_duration_ts: float
    start_delay_duration_ts: float
    post_charge_departure_delay_ts: float
    usage_departure_time_diff_ts: float
    duration_per_kwh_ts: float
    delivered_kwh: float
    kwh_request_diff: float
    kwh_per_usage_time: float

    # ê²°ì¸¡ì¹˜ í”Œë˜ê·¸
    charging_start_time_missing: bool
    charging_end_time_missing: bool
    expected_departure_time_missing: bool
    expected_usage_duration_missing: bool
    expected_time_diff_missing: bool
    actual_charging_duration_missing: bool
    start_delay_duration_missing: bool
    post_charge_departure_delay_missing: bool
    usage_departure_time_diff_missing: bool
    duration_per_kwh_missing: bool
    kwh_per_usage_time_missing: bool

    # ë²”ì£¼í˜•
    station_location: Optional[str]
    evse_name: Optional[str]
    evse_type: Optional[str]
    supports_discharge: Optional[str]

    # ì›í•« or ì •ìˆ˜í˜• ë²”ì£¼
    scheduled_charge: int
    weekday: int
    usage_departure_range: int
    post_charge_departure_range: int
    cluster: int
    requested_kwh : int


# ----------------------------------------
# âœ… ì„¤ì •ê°’ (ìˆ˜ì • í•„ìš”)
INPUT_SIZE = 12
HIDDEN_SIZE = 128
OUTPUT_SIZE = 1
SEQ_LENGTH = 10
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ----------------------------------------
# âœ… ëª¨ë¸ ë° ì§€ì› íŒŒì¼ ë¡œë“œ
lstm_model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE).to(DEVICE)
lstm_model.load_state_dict(torch.load("../model/lstm_model_best.pth", map_location=DEVICE))
lstm_model.eval()

temporal_model =TemporalFusionTransformer.load_from_checkpoint("../model/epoch=0-step=242.ckpt")
temporal_model.eval()



# âœ… í›ˆë ¨ ì‹œ ì €ì¥í•œ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
scaler = joblib.load("../model/input_scaler.pkl")          # ì…ë ¥ featureìš©
target_scaler = joblib.load("../model/target_scaler.pkl")   # íƒ€ê¹ƒ(requested_kwh) ìš©
numerical_columns = joblib.load("../model/numeric_cols.pkl")           # ì…ë ¥ ìˆ˜ì¹˜í˜•
expected_features = joblib.load("../model/expected_features.pkl")

# í•™ìŠµ ì‹œ ì…ë ¥ ìˆœì„œ ìœ ì§€ìš©
# âœ… MLP ëª¨ë¸ ë¡œë“œ
mlp_model = MLPModel(input_size=len(expected_features)).to(DEVICE)
mlp_model.load_state_dict(torch.load("../model/mlp_model_best.pth", map_location=DEVICE))
mlp_model.eval()


categorical_columns = [
    'evse_type','weekday'
]
one_hot_columns = joblib.load("../model/one_hot_columns.pkl")  

# ----------------------------------------
# âœ… FastAPI ì•± ì‹¤í–‰
app = FastAPI()

@app.get("/")
def index():
    return {"message": "EV Charging Prediction API ğŸ’¡"}

@app.post("/predict")
def predict(input_data: EVChargeAPIInput):
    try:
        # ì…ë ¥ ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ
        df = pd.DataFrame([input_data.dict()])
        df.columns = df.columns.str.strip()

        # ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in numerical_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0)

        # ë²”ì£¼í˜• ì›í•« ì¸ì½”ë”©
        df = pd.get_dummies(df, columns=categorical_columns, drop_first=False)

        # ëˆ„ë½ëœ expected_features ë³´ì™„ (0ìœ¼ë¡œ ì±„ì›€)
        for col in expected_features:
            if col not in df.columns:
                df[col] = 0

        # ìˆœì„œ ë§ì¶¤
        df = df[expected_features]

        # ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§
        inputs_for_scaling = [col for col in numerical_columns if col in df.columns]
        df[inputs_for_scaling] = scaler.transform(df[inputs_for_scaling])

        # LSTM ì…ë ¥: (1, seq_length, input_size)
        input_np = df.to_numpy().astype(np.float32)
        input_seq = np.repeat(input_np[np.newaxis, :, :], SEQ_LENGTH, axis=1)
        input_tensor = torch.tensor(input_seq).to(DEVICE)

        # ëª¨ë¸ ì˜ˆì¸¡ (ì •ê·œí™”ëœ ê°’)
        with torch.no_grad():
            normalized_output = lstm_model(input_tensor).cpu().numpy()[0, 0]

        # âœ… íƒ€ê¹ƒ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì—­ë³€í™˜
        denormalized_output = target_scaler.inverse_transform([[normalized_output]])[0, 0]

        return {
            "normalized_prediction": float(normalized_output),
            "denormalized_prediction": float(denormalized_output)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=repr(e))

@app.post("/predict/mlp")
def predict_mlp(input_data: EVChargeAPIInput):
    try:
        # ì…ë ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame([input_data.dict()])
        df.columns = df.columns.str.strip()

        # ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in numerical_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0)

        # ë²”ì£¼í˜• ì›í•« ì¸ì½”ë”©
        df = pd.get_dummies(df, columns=categorical_columns, drop_first=False)

        # ëˆ„ë½ëœ í”¼ì²˜ ë³´ì™„ (0ìœ¼ë¡œ)
        for col in expected_features:
            if col not in df.columns:
                df[col] = 0

        # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        df = df[expected_features]

        # ìŠ¤ì¼€ì¼ë§ (ìˆ˜ì¹˜í˜•ë§Œ)
        inputs_for_scaling = [col for col in numerical_columns]
        df[inputs_for_scaling] = scaler.transform(df[inputs_for_scaling])
        df = df.astype(np.float32)
        print("ğŸŒ DataFrame dtypes:")
        print(df.dtypes)

        # ğŸ¯ MLP ì…ë ¥ : (1, input_size)
        input_tensor = torch.tensor(df.values, dtype=torch.float32).to(DEVICE)

        # ì˜ˆì¸¡
        with torch.no_grad():
            normalized_output = mlp_model(input_tensor).item()

        denormalized_output = target_scaler.inverse_transform([[normalized_output]])[0][0]

        return {
            "normalized_prediction": float(normalized_output),
            "denormalized_prediction": float(denormalized_output),
            "model": "MLP"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=repr(e))
    
@app.post("/predict/temporl")
def predict(request: EVChargeAPIInput):
    # 1. ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬(DataFrame, Tensor ë“±)
    input_dict = request.dict()
    input_df = pd.DataFrame([input_dict])
    input_data = input_df  # ì…ë ¥ ë°ì´í„° í¬ë§·ì— ë§ê²Œ ë³€í™˜
    # 2. ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        prediction = temporal_model.predict(input_data)
    # 3. í›„ì²˜ë¦¬ ë° ê²°ê³¼ ë¦¬í„´
    return {"result": prediction.tolist()}